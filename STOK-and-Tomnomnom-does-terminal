STÖK and Tom does terminal
"VIM tutorial: linux terminal tools for bug bounty pentest and redteams with @tomnomnom"
___

Notes from STÖKs VIM tutorial/terminal tools with @tomnomnom, if you found this before the video for whatever reason it's here: https://www.youtube.com/watch?v=l8iXMgk2nnY

This is written for lazy people like me who would like to go back and find that thing in that video I know I saw but don't have time to look through again. Grep sadly doesn't work on videos nor voice yet. The below text might look like an instruction but says little if you did not watch the video so please do that. It might also be filled by typos or bad grammar, it's notes, correct me or get over it ;)
Other then that, enjoy (and once again, thanks to STÖK and Tom, Jesus you're good...)

### Update, got som comments, they are  marked with \[ comment \]

Create new folder for the project
mkdir <domain>

Find all subdomains, Tom built a tool for this, find Assetfinder on Github: https://github.com/tomnomnom

Pipe the output into a file:
assetfinder --subdomains-only domain.com > domains.txt

Check if the subdomains have web-pages on them (port 80and 443), again tool Tom built, whatweb would also work
cat domains | httpproble

To split the output into both on screen text and pipe into file use the tee command, that splits the output

It's important you call this file "hosts" for the next step
cat domains | httpproble | tee hosts

tee -A to append a file, this could be useful when you want to see your program is still running and hasn't hung

Another program Tom built is meg, it fetches loads of web-pages one at a time, you may set how often it fetches each site
Look for root of all the websites listed in hosts
meg -d 1000 -v /

There's a BURP tool called Turbo Intruder which is faster if you're looking at lots of stuff on one host.

When meg is done you'll get a folder called output, in that is one folder per host looked at.
In each folder there are two files, one for http and one for https

You can then grep from those files for subdomains and open the output in vim:
grep -Hnri <inser subdomain> * | vim -
// H display the filename
// n display the line number
// r recursive (goes into all directory's)
// i case insensitive

grep -vi, -v for invert and -i for case-insensitivity

In vim / is search, then n for next. Note-to-self, time to learn vim
In vim :%! %is current file and ! means run this in a shell, this pipes it into a process and then back. So for instance ":%!grep -v http" will get rid off all lines containing http \[You don't actually need to leave Vim for this: `:%g/http/d` works just as well. `g` means search, `http` is the pattern (regular expression), and `d` means delete. Trivia: grep has a funny origin story, see e.g. <https://youtu.be/EY6q5dv_B-o?t=2115>. The name grep comes from the ed command `g/re/p`, where again `g` means search, `re` is the pattern (regular expression), and `p` means print. ed is a predecessor of Vim.\]

Another tool by Tom is gf, takes files in ~/.gf/ (/home/USERNAME/.gf/) as argument, searches for anything matching the pattern described in the gf-file in the directory you're currently in.

gf arguments should be regexp. for grep on this format:
{
"flags": "-HnroE"
"pattern": "<insert pattern>"
}


Running gf is the same thing as running grep with the flags and pattern separately, only some patterns are really hard to remember, regexp and so on.

{" will become eyJ in base64, bonus-tip! \[You may recognise this from every JWT cookie.\]

gf base64 | vim -
:%!awk -F':' '{print $3}' or :%!cut -d ': ' -F3
This to separate into columns at : and then print field 3
awk handles things a bit better though
\[You can do this with a substitution, although it is a bit more involved: `:%s/^\([^:]\+: \)\{2\}\([^:]\+\)/\2/`.\]

Ctrl+V puts you into visual block mode (alt. mouse mark in notepad++), if you hit x \[or `d` for delete\] everything marked will get deleted.
Shift+A for append (end of row) then backspace, then esc, if you then hit down and . it will repeate whatever you did on the line below
%s/<insert search term>/ if you don't put anything in it will search for whatever you searched for last time
%s/<insert search term>/<insert replace term, empty for remove>/

Remove duplicates
:%!sort -u
\[The same can be done within Vim with `:%sort u`.\]

Temporarily exit vim (close window) How do you get back? \[You get back by typing `Ctrl-D`, which sends an end-of-file marker to the shell. I always do it the other way around, though. Assuming you started Vim from the shell, you can type `Ctrl-Z` to suspend Vim, do whatever you needed to do in the shell, and then type `fg` to get back into Vim.\]
:sh

To de-crypt base64, run:
echo <string> | base64 -d

Run a command for every line of a file
:%!xargs -n1 -I{} sh -c 'echo {} | base64 -d'
//n1 one at a time
//I{} place argument in here

Find all files
find . -type f

html-tool parses stuff out from files. It can also give you the content of tags. Of course also a Tom program
find . -type f | html-tool attribs src

To grep for files ending in .js pipe output to
grep '\.js$'

Output content of tags
find . -type f | html-tool tags title | vim -

Do grep for whateves under the cursor and open up results in new tab
:tabnew | read !grep -HNS 'self'

Take you back to the line
Ctrl+w then g then Shift+f

Turn json into javascript with gron (also Tom-tool, in default repo from ubuntu 19.04)
cat <json-file.json> | gron

Turn javascript into json
gron -u

Take everything that has ever been in a git repo and output it as one text-string
{ find .git/objects/pack/ -name "*.idx"|while read i;do git show-index < "$i"|awk '{print $2}';done;find .git/objects/ -type f|grep -v '/pack/'|awk -F'/' '{print $(NF-1)$NF}'; }|while read o;do git cat-file -p $o;done|grep -E 'pattern' 

> Gah my markdown reader in nextcloud needs this sorri


From there you can do a | grep -ai '<pattern to match>' \[There is definitely a better way to do this.\]

List every version of every file ever put on git.
With this you can list code that might have been removed (keys might be published by mistake and then 'removed')
ls .git/objects

You can of course delete stuff forever and wipe away all history of it, but it's harder then one could think: https://chrisshort.net/permanently-remove-any-record-of-a-file-from-git/

To get back in time in git-repository three commits (^ is one commit)
git checkout master^^^ \[alternatively `git checkout master~3`\]

To get back to present commit
git checkout -

ghdump.sh \[Cleaned it up a bit for you.\]
```
 0.  #!/bin/bash
 1.
 2.  {
 3.      find .git/objects/pack/ -name '*.idx' |
 4.      while read i; do
 5.          git show-index < "$i" |
 6.          awk '{print $2}';
 7.      done;
 8.      find .git/objects/ -type f |
 9.      grep -v '/pack/' |
10.      awk -F'/' '{print $(NF-1)$NF}';
11.  } |
12.  while read o; do
13.      git cat-file -p $o;
14. done
```
> gah once again

Lets look at each line
0. A shabang, tells kernel that this is a shell file that needs to be handled by bash \[Shebang means hash-bang. The hash must come before the bang.\]
3. find files in the repository archive that ends in .idx. Github will pack all object-files together for compression, this is called a pack-file. To know where one object ends and begins github uses .idx-files to save the of sets in the pack-files.
4. Puts each index-file into a variable called i
5. Take the file on the left, and attach it to the input of this command
5. The git show-index outputs three columns of data, we want the second. This command does this.
7. In total this part of the function gives us all object hashes that have been packed.
8. Now we want to find all objects that have not been listed already (all unpacked). First we list all files. The -type f means we don't list directories, only files.
9. Then we remove all we've already listed
10. -F in awk is a field separator, here we separate by / and then print the second to last field and the last field (number of fields minus one)(number of fields)
11. This is the end of the total function, which means we put them together and can pipe them into something else
12. We put the output into a variable called O. O now contains all information we need to do stuff with it, stored in memory.
13. We now cat (print) all the files
14. program \[actually while-loop\] done

\[Here's a slightly improved version of the script.\]
```
#!/bin/bash

{
    for i in .git/objects/pack/*.idx; do
        git show-index <$i | cut -d ' ' -f 2
    done
    find .git/objects/ \
        -type f \
        -path .git/objects/pack -prune |
    awk -F / '{print $(NF-1)$NF}'
} | xargs -n 1 git cat-file -p
```
> yeah....

We can now run ./ghdump.sh | grep '<pattern to match>'

This will, however, bring is some problems, "Binary file (standard input) matches". This means we've feed it some binary file, there might have been an image or something in the git-repository. grep -a stops this from happening since this instructs grep to treat binary files as text-files.

Let's find the urls
gf urls | grep -i <pattern to match> | vim -
:%!sort -u

Now let's use another Tom-tool to sort out unique paths
:%!unfurl -u paths

We might also wanna parse for query strings (https://en.wikipedia.org/wiki/Query_string)
:%!unfurl -u keys
This is really helpful if your running param miner extension for BURP (https://github.com/PortSwigger/param-miner). This would create a great word-list

To check waybackmachine for URLs use:
echo example.com | waybackurls | head

More about Tom here:
https://tomnomnom.com/
https://twitter.com/tomnomnom
https://github.com/tomnomnom

More about STÖK here:
https://www.youtube.com/channel/UCQN2DsjnYH60SFBIA6IkNwg
https://twitter.com/stokfredrik
